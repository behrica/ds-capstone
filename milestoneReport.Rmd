<style type="text/css">
.table {
   width: auto;
   max-width: none;
   border: 1px solid #ccc;
}
 
</style>


---
title: "Milestone report"
output:
  html_document:
    theme: cerulean
---
```{r echo=FALSE}
library(pander)
library(data.table)
library(tau)
library(ngram)
```



```{r cache=TRUE,echo=FALSE,result=FALSE}

my.read.lines2=function(fname) {
  s = file.info( fname )$size
  s = 20000000
  #s=20000
  
  buf = readChar( fname, s)
  lines <- strsplit(buf,"\n",fixed=T)[[1]]
}

extractNgrams <- function(x,n) {
  textcnt(x,n=n,method = "string",decreasing = T)
}

extractCounts <- function(files) {
  results = list()
  for (file in files) {
    text <- my.read.lines2(file)
    cnt_1 <- extractNgrams(text,1)
    cnt_2 <- extractNgrams(text,2)
    cnt_3 <- extractNgrams(text,3)
    cnt_4 <- extractNgrams(text,4)    
    results[[file]] <- list(gram1 = cnt_1,gram2=cnt_2,gram3=cnt_3,gram4=cnt_4)
  }
  results
}

results <- extractCounts(c("final//en_US/en_US.blogs.txt","final//en_US/en_US.news.txt","final//en_US/en_US.twitter.txt"))


```



# Purpose of Report
The purpose of this report is to give some characteristics of the data used for building a prediction model and application for the prediction of words.

# Description of dataset

## Structure of text corpus files
The input data for the prediction algorithm is a corpus of text, split in several files, which can be downloaded here:

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The files contains 3 text files for each of the four languages:


* en_US : English
* fi_FI : Finish
* de_DE : German
* ru_RU : Russian


For the purpose of this report we will concentrate on English.


The following table shows some characteristics of the size of the 3 English files:

file name             size in bytes      number of lines     number of words
---------             ------             ----------          ----------
en_US.blogs.txt       210.160.014           899.288          37.334.114
en_US.news.txt        205.811.889         1.010.242          34.365.936
en_US.twitter.txt     167.105.338         2.360.148          30.359.852
 
This data was obtained by running the Linux tool "wc" on the files. 
 
From this we can calculate the number of words per line, which is rather different between the files.

file name             avg. words per line      
---------             ---------            
en_US.blogs.txt       41
en_US.news.txt        34
en_US.twitter.txt     12

This matches the intuition, that twitter messages are very short, new and blogs are longer.

 
## Ngrams
 The next table shows the number of n-grams in the 3 different files.
 
 I tried several R packages for extracting n-grams from the complete files:
 - NgramTokenizer (package "RWeka")
 - textcnt (package "tau")
 - ngram (package "ngram" : https://github.com/wrathematics/ngram)
 
 All of my trials to tokenize(or extract ngrams) with the full files in R, even by using several strategies of splitting the text in blocks of lines, failed with memory problems.
 (On a 8 GB Ram server from Amazon EC2)
 
 The only tool I found, which reliable could generate n-grams from the full files, was "ngramtools" , a command line application available here:
 http://homepages.inf.ed.ac.uk/lzhang10/ngram.html
 
 
So finally I restricted the n-gram analysis to the first 20.000.000 bytes per file and ngrams of length 1,2,3 and 4.
This represents roughly 10% percent of the full text corpus.
The generation of the ngram tables for ~ 60 MB (3 * 20M) took only some minutes and generated a 1.4 GB  n-gram frequency table
 
 
Those numbers were obtained by using the "textcnt" functions from R-package "tau" on 10% of the text.


The results object has a size of `r format(object.size(results),units="b")`

```{r results='asis',echo=FALSE}
fileNames <- names(results)
ng1.count <- sapply(fileNames,FUN=function(fileName) length(results[[fileName]][[1]]),USE.NAMES = F)
ng2.count <- sapply(fileNames,FUN=function(fileName) length(results[[fileName]][[2]]),USE.NAMES = F)
ng3.count <- sapply(fileNames,FUN=function(fileName) length(results[[fileName]][[3]]),USE.NAMES = F)
ng4.count <- sapply(fileNames,FUN=function(fileName) length(results[[fileName]][[4]]),USE.NAMES = F)
df <- data.frame("file name"=c("blogs","news","twitter"),"gram.1"=ng1.count,"gram.2"=ng2.count,"gram.3"=ng3.count,"grams.4"=ng4.count)
pandoc.table(df,caption="N-gram counts",style="simple")
```

This table shows, that the Twitter text has the least number of different n-grams, so the most reduced vocabulary.

## most frequent words



```{r results='asis',echo=FALSE}
pandoc.table(head(format(results[[1]][[1]])[,1:2]),caption="Most frequent words in 'blogs'",style="simple")
pandoc.table(head(format(results[[2]][[1]])[,1:2]),caption="Most frequent words in 'news'",style="simple")
pandoc.table(head(format(results[[3]][[1]])[,1:2]),caption="Most frequent words in 'twitter'",style="simple")

```



An interesting difference is here, is that in the twitter text "I" and "you" is higher ranking.
This reflect the nature of personal messages in Twitter tweets.

## Prediction model
It became clear, that a trade-of need to be made between the computation time/memory consumption for creating the model and versus prediction accuracy need to be made.

The "principle" of the prediction algorithm to be implemented can look like this:

1. Given a frequency table of n-grams, which lists per n-gram the counts (or probability)
2. Given a "n-gram" x of length "m" for which the next word "y"" should be predicted
3. Find counts in frequency table of all n-grams of length m-1 where the first words match with x
4. If found in table, select the one with highest count and use as predicted next word "y"
5. If none found in step 3, -> "step back" and reduce m by one (from the left) and go to step 3
6. If none found, use word "the" as prediction(as it is overall the most frequent word in English language)

As more complete our frequency table is, as more reliable the prediction would be.

This algorithm does not take the context apart from the previous words into consideration.


A very important consideration is as well, which n-grams(n=1,2,3,4....) should be saved. As larger the "n" gets, as less "predictive power" each n gram has, because it is less likely that it will ever be used for prediction and the number of potential n-grams increases exponentially per "n".

A further question is, if "low" counts (0,1,2 ...) should be stored at all ?  If a-ngram is very unlikely to occur in training text, it is unlikely that user will ever want to type it. A very large number of n-grams ocur only ones.
For example, the percentage of bigrams with count=2 in blogs is 
```{r echo=FALSE}
blogs_2Grams <- results[[1]][[2]]
length(blogs_2Grams[blogs_2Grams==1]) / length(blogs_2Grams)
```



This is seen as well in the following plot. It plots teh cumualted sum of occurencies versus teh number of terms for the "blogs" and teh bigrams:
```{r cache=TRUE,echo=FALSE}
 occurencies <- sum(results[[1]][[2]])
 plot(cumsum(results[[1]][[2]])/occurencies,xlab="rank",ylab="cumulative sum of occurencies")
```

So by storing only about 200.000 out of 1.200.000 bigrams, we get already a 75% coverage on the training text
For hihern "n" this gets worse, so we need more and more storage to get high coverage.


So in summary, we have those parameters to tune the process:

For building the frequency table:

- corpus size used for building the frequency table
- which ngrams to extract and store (,2,3,4 ...) ?
- which frequencies to include in table (> 3 ) ?
. use more context the n-grams ?


For using the frequency table:

- effective search in the table (avoid full text scan, if possible, by using a kind of tree) 














